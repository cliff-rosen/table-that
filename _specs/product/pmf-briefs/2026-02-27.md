# PMF Director — Priority Brief

**Date:** 2026-02-27
**Brief #:** 1
**Focus:** Full review (inaugural run)

## Current State Assessment

The core product loop (Build → Populate → Enrich) is fully implemented and deployed to production. 14 tools registered, 3 pages with full chat integration, enrichment strategies working with value coercion, import/export functional, landing page live. The product is feature-complete enough for real users. The biggest risk is not missing features — it's that we have no users and no distribution mechanism. We're building in a vacuum with no feedback signal. Event tracking infrastructure exists but isn't measuring the things that matter (journey stages, completion rates). Test suite is 50% failing due to a blocking bug.

## Inputs Reviewed

| Input | Status | Notes |
|-------|--------|-------|
| Product spec | Read | Core features all implemented: tables, chat, proposals, tools, import/export |
| PMF criteria | Read | Target user defined, 6 launch blockers identified (all met), success measures defined but unmeasured |
| Verticals doc (Part 5) | Read | 3 strategies + coercion built. Challenges 1, 2 solved. 3, 5, 6, 7 not started. |
| Roadmap | Read | 22 open items (2 resolved). 13 P1s — needs triage. |
| Codebase scan | Done | 14 tools, 3 page configs, 3 core pages, deployment automated |
| Git log | Read | Heavy development, 20+ commits, mostly "backup" commit messages |
| Test coverage | Checked | 4 test files, 3/6 tests failing (blocking bug in row service: "cannot access local variable 'out'") |
| Usage analytics | Partial | EventTrackingService exists with admin query API. But no journey stage tracking, no funnel metrics. |
| User feedback | Missing | No real users yet. No feedback channel. |
| QA results | Missing | No recent walkthrough. Should run one. |

## Top 3 Recommendations

### 1. Build shareable tables (the distribution engine)

**Category:** GROWTH
**Roadmap items:** #24
**Why this matters for PMF:** Without distribution, PMF is impossible. Right now the only way someone sees table.that is if you directly tell them. Shareable tables turn every user into a distribution channel. A restaurant list texted to friends, a vendor comparison emailed to a team — each one is a zero-cost product demo viewed by someone who has a reason to care. The PMF criteria identifies shared table views and fork rate as lagging indicators, but they can't be measured until sharing exists.

**Minimum viable scope:**
- Backend: `share_token` field on tables, `GET /api/public/tables/{share_token}` endpoint (no auth required), returns read-only table data
- Frontend: Share button on TableViewPage generates link, public read-only TableViewPage (no chat, no edit), "Fork this table" CTA for signed-in users, "Sign up to make your own" for anonymous
- Skip for now: expiry, unlisted mode, comments, permissions UI

**Estimated effort:** 2-3 days

### 2. Wire up journey stage tracking

**Category:** GROWTH
**Roadmap items:** #23
**Why this matters for PMF:** The PMF criteria defines 4 leading indicators (time to first table, research accuracy, completion rate, return rate) but none are being measured. Event tracking infrastructure already exists (EventTrackingService + admin API) — we just need to emit the right events. Without this, we can't tell if users complete the loop, where they drop off, or whether changes we make improve anything. Flying blind.

**Minimum viable scope:**
- Define 4 stage events: `table_stage_build`, `table_stage_populate`, `table_stage_organize`, `table_stage_enrich`
- Emit from existing code paths: table creation (build), data proposal apply (populate), sort/filter interaction (organize), enrich_column completion (enrich)
- Add `table_id` and `timestamp` to event data
- Query via existing admin tracking API — no new dashboard needed initially

**Estimated effort:** 1 day (infrastructure exists, just need event emission points)

### 3. Fix test failures and run QA stabilization

**Category:** QUALITY
**Roadmap items:** #9, #21
**Why this matters for PMF:** 50% of existing tests are failing due to a bug in the row service ("cannot access local variable 'out'"). The 403 defect means enrichment breaks on bot-protected sites. Before putting the product in front of real users, these need to be fixed. First impressions are permanent — a user who hits broken enrichment on their first try won't come back.

**Minimum viable scope:**
- Fix the row service variable bug (blocking 3 test failures)
- Run `/qa-walkthrough` to identify current rough edges
- Fix any blockers the QA finds in the core loop
- Don't try to build full test coverage (#13) — just fix what's broken

**Estimated effort:** 1-2 days

## Deprioritize

| Item | Current P | Suggested P | Reasoning |
|------|-----------|-------------|-----------|
| #15 Vertical-specific tooling | P1 | P2 | Generic strategies cover Tier 1 verticals. This is a progressive enhancement, not a launch requirement. |
| #16 Domain tool packs | P1 | P2 | Same as #15 — nice to have, not needed for PMF validation. |
| #17 Entity type system | P1 | P2 | Progressive enhancement. Coercion layer handles type fitting without entity types. |
| #19 Recommendations via SerpAPI | P1 | P2 | Population works via research_web without this. Optimization, not a blocker. |
| #20 Persistent jobs | P1 | P2 | Matters after we prove users complete the loop. Enrichment works synchronously for tables under 20 rows. |
| #22 Direct update policy | P1 | P2 | Real issue but not user-facing enough to be a launch blocker. |
| #14 AI-driven dev automation | P1 | P3 | Meta-feature. Building the product is more important than automating the building of the product. |
| #7 Email/SMS integration | P2 | P3 | Speculative. No user signal suggesting this is wanted. |
| #6 Mobile experience | P2 | P3 | Table creation and enrichment are desktop workflows. Mobile viewing could matter for shared tables, but build sharing first. |

## Gaps

Things not on the roadmap that should be:

1. **First-time user onboarding** (CORE, P1) — When a new user signs up, they land on an empty tables list with a chat panel. There's no guidance, no suggested first prompt, no template gallery. The time-to-first-value metric depends entirely on the user figuring out what to type. A single suggested prompt ("Try: 'Build me a list of...'") or a template gallery could dramatically reduce drop-off.

2. **Distribution plan beyond product** (GROWTH, P2) — Shareable tables enable organic distribution, but someone has to create the first shared tables. How do the first 50 users find table.that? Demo videos exist but aren't published anywhere. No social media, no content marketing, no outreach. This doesn't need to be on the roadmap as a feature — it's a task.

3. **Enrichment error UX** (CORE, P2) — When for_each_row research fails for a row (timeout, 403, no results), what does the user see? Is it clear what happened and what they can do? The research log captures failures, but the DataProposalCard UX for failed rows may not be clear enough.

## Blockers

- **No real users** — Every recommendation above is theoretical until real people try the product. The #1 meta-blocker is distribution, which is why shareable tables is recommendation #1.
- **Test failures** — 3/6 tests failing. Can't ship changes confidently. The row service bug needs to be fixed before anything else.
- **No QA baseline** — No recent walkthrough means we don't know what's currently broken in the live product.

## Roadmap Health

- **Total open items:** 22 (2 resolved: #2, #8)
- **By category:** CORE: 4, GROWTH: 3, QUALITY: 5, INFRA: 2, AI: 4, META: 2
- **Items that should be resolved:** #2 (strategy framework — built), #8 (research thresholds — built). Already marked done.
- **Priority distribution (current):** P1: 13, P2: 7, P3: 0
- **Priority distribution (proposed):** P1: 5, P2: 10, P3: 3

**Assessment:** The roadmap has too many P1s. 13 out of 20 open items are P1, which means nothing is actually prioritized. After triage, only 5 items should be P1: the three recommendations above (#24, #23, QA fixes) plus #3 (enrichment results UX) and #4 (rich table display) which directly affect whether users have a good experience in the core loop. Everything else is either already working (demote to done), a progressive enhancement (P2), or speculative (P3).
